{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30458,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic competition with TensorFlow Decision Forests\n\nThis notebook will take you through the steps needed to train a baseline Gradient Boosted Trees Model using TensorFlow Decision Forests and creating a submission on the Titanic competition. \n\nThis notebook shows:\n\n1. How to do some basic pre-processing. For example, the passenger names will be tokenized, and ticket names will be splitted in parts.\n1. How to train a Gradient Boosted Trees (GBT) with default parameters\n1. How to train a GBT with improved default parameters\n1. How to tune the parameters of a GBTs\n1. How to train and ensemble many GBTs","metadata":{}},{"cell_type":"markdown","source":"Gradient Boosted Treesï¼ˆæ¢¯åº¦æå‡æ ‘ï¼Œç®€ç§°GBTæˆ–GBDTï¼‰æ˜¯ä¸€ç§é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œä¸»è¦ç”¨äºå›å½’å’Œåˆ†ç±»ä»»åŠ¡ã€‚å®ƒé€šè¿‡**é›†æˆå¤šä¸ªå¼±å­¦ä¹ å™¨ï¼ˆé€šå¸¸æ˜¯å†³ç­–æ ‘ï¼‰**æ¥å½¢æˆä¸€ä¸ªå¼ºå­¦ä¹ å™¨ã€‚è¿™é‡Œæ˜¯ä¸€ä¸ªä¿¡æ¯å¯†åº¦é«˜çš„è§£é‡Šï¼š\n\næ ¸å¿ƒæ€æƒ³\nè¿­ä»£è®­ç»ƒï¼š æ¯ä¸€è½®è®­ç»ƒä¸€ä¸ªæ–°æ ‘ï¼Œç”¨æ¥çº æ­£ä¹‹å‰æ¨¡å‹çš„æ®‹å·®ï¼ˆå³é¢„æµ‹è¯¯å·®ï¼‰ã€‚\n\nåŠ æƒæ±‚å’Œï¼š æ‰€æœ‰æ ‘çš„é¢„æµ‹ç»“æœåŠ æƒç›¸åŠ ï¼Œå¾—åˆ°æœ€ç»ˆé¢„æµ‹å€¼ã€‚\n\næ¢¯åº¦ä¸‹é™ï¼š åˆ©ç”¨æŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼ŒæŒ‡å¯¼æ¯æ£µæ–°æ ‘å¦‚ä½•æ‹Ÿåˆæ®‹å·®ã€‚","metadata":{}},{"cell_type":"markdown","source":"# Imports dependencies","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nimport tensorflow as tf\nimport tensorflow_decision_forests as tfdf\n\nprint(f\"Found TF-DF {tfdf.__version__}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:32:04.264037Z","iopub.execute_input":"2023-04-17T15:32:04.26451Z","iopub.status.idle":"2023-04-17T15:32:04.272355Z","shell.execute_reply.started":"2023-04-17T15:32:04.264459Z","shell.execute_reply":"2023-04-17T15:32:04.270819Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load dataset","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\nserving_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\ntrain_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:30:04.102889Z","iopub.execute_input":"2023-04-17T15:30:04.10358Z","iopub.status.idle":"2023-04-17T15:30:04.158126Z","shell.execute_reply.started":"2023-04-17T15:30:04.103539Z","shell.execute_reply":"2023-04-17T15:30:04.156877Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare dataset\n\nWe will apply the following transformations on the dataset.\n\n1. Tokenize the names. For example, \"Braund, Mr. Owen Harris\" will become [\"Braund\", \"Mr.\", \"Owen\", \"Harris\"].\n2. Extract any prefix in the ticket. For example ticket \"STON/O2. 3101282\" will become \"STON/O2.\" and 3101282.","metadata":{}},{"cell_type":"code","source":"def preprocess(df):\n    df = df.copy()\n    \n    def normalize_name(x):\n        return \" \".join([v.strip(\",()[].\\\"'\") for v in x.split(\" \")])\n    \n    def ticket_number(x):\n        return x.split(\" \")[-1]\n        \n    def ticket_item(x):\n        items = x.split(\" \")\n        if len(items) == 1:\n            return \"NONE\"\n        return \"_\".join(items[0:-1])\n    \n    df[\"Name\"] = df[\"Name\"].apply(normalize_name)\n    df[\"Ticket_number\"] = df[\"Ticket\"].apply(ticket_number)\n    df[\"Ticket_item\"] = df[\"Ticket\"].apply(ticket_item)                     \n    return df\n    \npreprocessed_train_df = preprocess(train_df)\npreprocessed_serving_df = preprocess(serving_df)\n\npreprocessed_train_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:30:09.50201Z","iopub.execute_input":"2023-04-17T15:30:09.502467Z","iopub.status.idle":"2023-04-17T15:30:09.540151Z","shell.execute_reply.started":"2023-04-17T15:30:09.502432Z","shell.execute_reply":"2023-04-17T15:30:09.538948Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's keep the list of the input features of the model. Notably, we don't want to train our model on the \"PassengerId\" and \"Ticket\" features.","metadata":{}},{"cell_type":"code","source":"input_features = list(preprocessed_train_df.columns)\ninput_features.remove(\"Ticket\")\ninput_features.remove(\"PassengerId\")\ninput_features.remove(\"Survived\")\n#input_features.remove(\"Ticket_number\")\n\nprint(f\"Input features: {input_features}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:30:14.000466Z","iopub.execute_input":"2023-04-17T15:30:14.000868Z","iopub.status.idle":"2023-04-17T15:30:14.00835Z","shell.execute_reply.started":"2023-04-17T15:30:14.000833Z","shell.execute_reply":"2023-04-17T15:30:14.006982Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Convert Pandas dataset to TensorFlow Dataset","metadata":{}},{"cell_type":"code","source":"def tokenize_names(features, labels=None):\n    \"\"\"Divite the names into tokens. TF-DF can consume text tokens natively.\"\"\"\n    features[\"Name\"] =  tf.strings.split(features[\"Name\"])\n    return features, labels\n\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_train_df,label=\"Survived\").map(tokenize_names)\nserving_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_serving_df).map(tokenize_names)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:30:17.896317Z","iopub.execute_input":"2023-04-17T15:30:17.896741Z","iopub.status.idle":"2023-04-17T15:30:18.231195Z","shell.execute_reply.started":"2023-04-17T15:30:17.896705Z","shell.execute_reply":"2023-04-17T15:30:18.229792Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"features æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯åˆ—åï¼Œå€¼æ˜¯å¼ é‡ã€‚\n\ntf.strings.split(features[\"Name\"]) ä¼šå°†å§“åå­—ç¬¦ä¸²æŒ‰ç©ºæ ¼æ‹†åˆ†æˆåˆ—è¡¨ï¼ˆè¯ tokensï¼‰ï¼Œå¦‚ \"Smith, John\" -> [\"Smith,\", \"John\"]\n\nTF-DF å¯ä»¥ç›´æ¥ä½¿ç”¨ token åˆ—ä½œä¸ºæ–‡æœ¬ç‰¹å¾ï¼Œä¸åƒä¸€èˆ¬çš„æ¨¡å‹éœ€è¦åš embeddingã€‚\n\næœ€åè¿”å›ä¿®æ”¹åçš„ features å’Œæ ‡ç­¾ labelsï¼ˆå¦‚æœæœ‰ï¼‰ã€‚\n\npd_dataframe_to_tf_dataset(...)ï¼šæŠŠ pandas çš„ DataFrame è½¬æˆ TensorFlow çš„æ•°æ®é›†ã€‚\n\nlabel=\"Survived\"ï¼šæŒ‡å®šç›®æ ‡å˜é‡ã€‚\n\n.map(tokenize_names)ï¼šå¯¹æ•°æ®é›†çš„æ¯ä¸€æ¡è®°å½•åº”ç”¨ tokenize_names å‡½æ•°ï¼Œå³è¿›è¡Œå§“å token åŒ–ã€‚","metadata":{}},{"cell_type":"markdown","source":"# Train model with default parameters\n\n### Train model\n\nFirst, we are training a GradientBoostedTreesModel model with the default parameters.","metadata":{}},{"cell_type":"code","source":"model = tfdf.keras.GradientBoostedTreesModel(\n    verbose=0, # Very few logs\n    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n    exclude_non_specified_features=True, # Only use the features in \"features\"\n    random_seed=1234,\n)\nmodel.fit(train_ds)\n\nself_evaluation = model.make_inspector().evaluation()\nprint(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:32:16.580089Z","iopub.execute_input":"2023-04-17T15:32:16.581306Z","iopub.status.idle":"2023-04-17T15:32:17.55132Z","shell.execute_reply.started":"2023-04-17T15:32:16.581257Z","shell.execute_reply":"2023-04-17T15:32:17.55024Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train model with improved default parameters\n\nNow you'll use some specific parameters when creating the GBT model","metadata":{}},{"cell_type":"code","source":"model = tfdf.keras.GradientBoostedTreesModel(\n    verbose=0, # Very few logs\n    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n    exclude_non_specified_features=True, # Only use the features in \"features\"\n    \n    #num_trees=2000,\n    \n    # Only for GBT.\n    # A bit slower, but great to understand the model.\n    # compute_permutation_variable_importance=True,\n    \n    # Change the default hyper-parameters\n    # hyperparameter_template=\"benchmark_rank1@v1\",\n    \n    #num_trees=1000,\n    #tuner=tuner\n    \n    min_examples=1,\n    categorical_algorithm=\"RANDOM\",\n    #max_depth=4,\n    shrinkage=0.05,\n    #num_candidate_attributes_ratio=0.2,\n    split_axis=\"SPARSE_OBLIQUE\",\n    sparse_oblique_normalization=\"MIN_MAX\",\n    sparse_oblique_num_projections_exponent=2.0,\n    num_trees=2000,\n    #validation_ratio=0.0,\n    random_seed=1234,\n    \n)\nmodel.fit(train_ds)\n\nself_evaluation = model.make_inspector().evaluation()\nprint(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:06:14.822939Z","iopub.execute_input":"2023-04-17T15:06:14.82343Z","iopub.status.idle":"2023-04-17T15:06:16.103565Z","shell.execute_reply.started":"2023-04-17T15:06:14.82339Z","shell.execute_reply":"2023-04-17T15:06:16.102272Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### è¶…å‚æ•°ï¼ˆé‡ç‚¹éƒ¨åˆ†ï¼‰\nmin_examples=1\næ¯ä¸ªå¶èŠ‚ç‚¹æœ€å°‘æ ·æœ¬æ•°ï¼Œè®¾ç½®ä¸º 1 æ„å‘³ç€å…è®¸å¾ˆæ·±çš„æ ‘ï¼ˆå¯èƒ½è¿‡æ‹Ÿåˆï¼Œä½†é€‚åˆå­¦ä¹ å¤æ‚æ¨¡å¼ï¼‰ã€‚\n\ncategorical_algorithm=\"RANDOM\"\nåˆ†ç±»å˜é‡åˆ†è£‚ç­–ç•¥ä¸ºéšæœºé€‰æ‹©ï¼ˆé€šå¸¸ç”¨äºå¤§ç±»å˜é‡ï¼‰ã€‚\n\nshrinkage=0.05\nå­¦ä¹ ç‡ï¼ˆåˆå shrinkageï¼‰ï¼Œæ§åˆ¶æ¯æ£µæ ‘å¯¹æœ€ç»ˆé¢„æµ‹çš„è´¡çŒ®ï¼Œè¶Šå°è¶Šç¨³å®šä½†è®­ç»ƒæ…¢ã€‚\n\nsplit_axis=\"SPARSE_OBLIQUE\"\nä½¿ç”¨ç¨€ç–æ–œç‡åˆ†è£‚è½´ï¼ˆSparse Oblique Splitï¼‰ï¼šå…è®¸ç”¨å¤šä¸ªç‰¹å¾çš„çº¿æ€§ç»„åˆè¿›è¡Œåˆ†è£‚ï¼Œè€Œéä¸€ä¸ªç‰¹å¾ä¸€ä¸ªåˆ†è£‚ï¼ˆæ›´å¤æ‚çš„æ¨¡å‹èƒ½åŠ›ï¼‰ã€‚\n\nsparse_oblique_normalization=\"MIN_MAX\"\nå¯¹çº¿æ€§ç»„åˆä¸­çš„ç‰¹å¾è¿›è¡Œ Min-Max å½’ä¸€åŒ–ã€‚\n\nsparse_oblique_num_projections_exponent=2.0\næ§åˆ¶ç”Ÿæˆå¤šå°‘ç§ç»„åˆåˆ†è£‚æ–¹å¼ã€‚è¿™ä¸ªå€¼æ˜¯æŒ‡æ•°ï¼Œå®é™…æ˜¯ \\text{num_features}^{2.0} ä¸ªç»„åˆã€‚\n\nnum_trees=2000\nè®­ç»ƒ 2000 æ£µæ ‘ï¼Œæ¨¡å‹è¶Šå¤æ‚ï¼Œæ‹Ÿåˆèƒ½åŠ›è¶Šå¼ºã€‚\n\nrandom_seed=1234\nå›ºå®šéšæœºç§å­ï¼Œä½¿ç»“æœå¯å¤ç°ã€‚","metadata":{}},{"cell_type":"markdown","source":"### 1. categorical_algorithm=\"RANDOM\"\nè¿™æ˜¯æŒ‡å®šç±»åˆ«ç‰¹å¾å¦‚ä½•åˆ†è£‚ï¼ˆå³å†³ç­–æ ‘ä¸­å¦‚ä½•å¤„ç†å­—ç¬¦ä¸²ç±»å‹å˜é‡ï¼‰çš„æ–¹å¼ä¹‹ä¸€ã€‚TF-DF æ”¯æŒå‡ ç§ç®—æ³•ï¼Œ\"RANDOM\" æ˜¯å…¶ä¸­è¾ƒç‰¹æ®Šçš„ä¸€ç§ã€‚\n\nä½œç”¨ï¼š\nå¯¹äºæŸä¸ªç±»åˆ«ç‰¹å¾ï¼Œæ ‘åœ¨é€‰æ‹©åˆ†è£‚ç‚¹æ—¶ä¸ä¼šæšä¸¾æ‰€æœ‰å¯èƒ½çš„å­é›†ï¼Œè€Œæ˜¯éšæœºæŒ‘é€‰ä¸€äº›å­é›†è¿›è¡Œè¯„ä¼°ã€‚\n\nä¸ºä»€ä¹ˆè¿™æ ·åšï¼Ÿ\næšä¸¾æ‰€æœ‰å­é›†ä»£ä»·éå¸¸é«˜ï¼šå¦‚æœä¸€ä¸ªç‰¹å¾æœ‰ 20 ä¸ªç±»åˆ«ï¼Œç†è®ºä¸Šæœ‰ \n2^19-1=524,287 ç§å¯èƒ½çš„åˆ†è£‚æ–¹å¼ã€‚\n\nä½¿ç”¨ \"RANDOM\" ç®—æ³•å¯æ˜¾è‘—å‡å°‘è®¡ç®—é‡ï¼Œå°¤å…¶åœ¨ç±»åˆ«æ•°é‡å¤šçš„æ—¶å€™ã€‚\n\né€‚ç”¨åœºæ™¯ï¼š\nç‰¹å¾ç±»åˆ«æ•°å¤šï¼ˆä¾‹å¦‚åå­—ã€åœ°åï¼‰\n\næ•°æ®é‡å¤§ï¼Œè®­ç»ƒé€Ÿåº¦é‡è¦\n\nä¸è¿½æ±‚ç²¾ç¡®æœ€ä¼˜åˆ†è£‚ç‚¹ï¼Œåªæ±‚è¿‘ä¼¼å¥½ç»“æœ","metadata":{}},{"cell_type":"markdown","source":"### 2. sparse_oblique_normalization=\"MIN_MAX\"\nè¿™ä¸ªå‚æ•°æ˜¯åœ¨ä½¿ç”¨ \"SPARSE_OBLIQUE\" åˆ†è£‚è½´æ—¶ï¼ŒæŒ‡å®šçº¿æ€§ç»„åˆä¸­ç”¨åˆ°çš„æ•°å€¼ç‰¹å¾å¦‚ä½•å½’ä¸€åŒ–ã€‚\n\nèƒŒæ™¯ï¼šSPARSE_OBLIQUE\nç¨€ç–æ–œç‡ï¼ˆsparse obliqueï¼‰æ˜¯å¯¹æ ‡å‡† axis-aligned å†³ç­–æ ‘çš„å¢å¼ºï¼Œå®ƒå…è®¸ç”¨å¤šä¸ªç‰¹å¾çš„çº¿æ€§ç»„åˆè¿›è¡Œå†³ç­–\n\nå¤šä¸ªç‰¹å¾çš„çº¿æ€§ç»„åˆç”¨äºèŠ‚ç‚¹åˆ†è£‚ï¼š\n\n$w_1 x_1 + w_2 x_2 + \\cdots + w_k x_k < \\text{threshold}$\n\n\nå…¶ä¸­ï¼š\n- \\( w_i \\)ï¼šæ¯ä¸ªç‰¹å¾çš„æƒé‡\n- \\( x_i \\)ï¼šç‰¹å¾å€¼\n- \\( k \\)ï¼šç‰¹å¾æ•°é‡\n\n\nä¸ºäº†è®©è¿™äº›ç»„åˆæ›´ç¨³å®šï¼Œéœ€è¦å¯¹ç‰¹å¾åšæ ‡å‡†åŒ–ã€‚\n\n\"MIN_MAX\" çš„å«ä¹‰ï¼š\næ¯ä¸ªç‰¹å¾ä¼šæŒ‰å¦‚ä¸‹æ–¹å¼ç¼©æ”¾ï¼š\nä½¿ç”¨æœ€å°-æœ€å¤§å½’ä¸€åŒ–å¯¹ç‰¹å¾ \\( x \\) åšç¼©æ”¾ï¼š\n\n$$\nx' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n$$\n\nå½’ä¸€åŒ–åæœ‰ï¼š\n\n$$\nx' \\in [0, 1]\n$$\n\n\nè¿™æ ·æ‰€æœ‰ç‰¹å¾çš„å–å€¼éƒ½è¢«å½’ä¸€åŒ–åˆ° [0, 1] èŒƒå›´å†…ã€‚\n\nç›®çš„ï¼š\nè®©ä¸åŒå°ºåº¦çš„ç‰¹å¾åœ¨ç»„åˆæ—¶æƒé‡æ›´åŠ å¯æ¯”\n\né¿å…æŸäº›å–å€¼èŒƒå›´å¤§çš„ç‰¹å¾ä¸»å¯¼åˆ†è£‚æ–¹å‘\n\nå¯é€‰é¡¹ï¼š\nè¿˜æœ‰æ¯”å¦‚ \"Z_SCORE\"ï¼ˆæ ‡å‡†å·®å½’ä¸€ï¼‰ç­‰æ–¹å¼ï¼Œä½† MIN_MAX æ›´ç›´è§‚ä¸”é€‚ç”¨äºå¤šæ•°åœºæ™¯ã€‚\n\næ€»ç»“ä¸€å¥è¯ï¼š\ncategorical_algorithm=\"RANDOM\" æ˜¯è®©é«˜åŸºæ•°ç±»åˆ«å˜é‡åˆ†è£‚æ›´é«˜æ•ˆï¼›\n\nsparse_oblique_normalization=\"MIN_MAX\" æ˜¯ä¿è¯æ–œç‡åˆ†è£‚ä¸­å¤šç‰¹å¾ç»„åˆçš„å°ºåº¦ç»Ÿä¸€æ€§ã€‚","metadata":{}},{"cell_type":"markdown","source":"Let's look at the model and you can also notice the information about variable importance that the model figured out","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:06:16.104878Z","iopub.execute_input":"2023-04-17T15:06:16.105239Z","iopub.status.idle":"2023-04-17T15:06:16.123439Z","shell.execute_reply.started":"2023-04-17T15:06:16.105206Z","shell.execute_reply":"2023-04-17T15:06:16.121993Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"model.summary() æ˜¯ TensorFlow / Keras ä¸­å¸¸ç”¨çš„æ–¹æ³•ä¹‹ä¸€ï¼Œä½œç”¨æ˜¯è¾“å‡ºæ¨¡å‹ç»“æ„æ‘˜è¦ã€‚ä¸è¿‡ï¼Œåœ¨ä½ ä½¿ç”¨çš„ TensorFlow Decision Forests (TF-DF) ä¸­ï¼Œå®ƒçš„ä½œç”¨ç¨æœ‰ä¸åŒã€‚\n\nåœ¨ TF-DF ä¸­ model.summary() çš„ä½œç”¨ï¼š\nå®ƒä¼šæ‰“å°å‡ºä¸€ä¸ªç®€ç•¥çš„æ¨¡å‹ç»“æ„ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š\n\næ¨¡å‹ç±»å‹ï¼ˆå¦‚ GradientBoostedTreesModelï¼‰\n\næ ‘çš„æ•°é‡ã€æ·±åº¦ã€èŠ‚ç‚¹æ•°ç­‰\n\nç‰¹å¾ä½¿ç”¨æƒ…å†µï¼ˆä¾‹å¦‚ç”¨äº†å“ªäº›ç‰¹å¾ï¼‰\n\nä¸€äº›è¶…å‚æ•°è®¾ç½®\n\n","metadata":{}},{"cell_type":"markdown","source":"# Make predictions","metadata":{}},{"cell_type":"code","source":"def prediction_to_kaggle_format(model, threshold=0.5):\n    proba_survive = model.predict(serving_ds, verbose=0)[:,0]\n    return pd.DataFrame({\n        \"PassengerId\": serving_df[\"PassengerId\"],\n        \"Survived\": (proba_survive >= threshold).astype(int)\n    })\n\ndef make_submission(kaggle_predictions):\n    path=\"/kaggle/working/submission.csv\"\n    kaggle_predictions.to_csv(path, index=False)\n    print(f\"Submission exported to {path}\")\n    \nkaggle_predictions = prediction_to_kaggle_format(model)\nmake_submission(kaggle_predictions)\n!head /kaggle/working/submission.csv","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:06:16.126575Z","iopub.execute_input":"2023-04-17T15:06:16.126941Z","iopub.status.idle":"2023-04-17T15:06:17.406876Z","shell.execute_reply.started":"2023-04-17T15:06:16.126904Z","shell.execute_reply":"2023-04-17T15:06:17.405022Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### model.predict(serving_ds, verbose=0)\nserving_ds:\nä¸€ä¸ª TensorFlow Datasetï¼Œä¸å«æ ‡ç­¾ï¼Œç”¨äºé¢„æµ‹ã€‚ä¸€èˆ¬æ˜¯æµ‹è¯•é›†æˆ–éƒ¨ç½²ç”¨æ•°æ®ã€‚\n\nverbose=0:\nè®¾ç½®æ—¥å¿—ç­‰çº§ä¸ºâ€œé™é»˜â€ï¼Œä¸è¾“å‡ºä»»ä½•è¿›åº¦æ¡æˆ–é¢„æµ‹æ—¥å¿—ã€‚å¸¸ç”¨äºè„šæœ¬æ‰§è¡Œæˆ– notebook å¹²å‡€è¾“å‡ºã€‚\n\n[:, 0] çš„ä½œç”¨\n.predict(...) è¿”å›çš„æ˜¯ä¸€ä¸ª NumPy æ•°ç»„ï¼Œå½¢çŠ¶å¤§è‡´ä¸º (num_samples, num_classes)ã€‚\n\nå¯¹äºäºŒåˆ†ç±»é—®é¢˜ï¼ˆå¦‚ Titanic æ˜¯å¦ç”Ÿå­˜ï¼‰ï¼Œå®ƒè¿”å›çš„æ˜¯æ¯ä¸ªæ ·æœ¬çš„ä¸¤ä¸ªæ¦‚ç‡ï¼š\n\n[[0.75, 0.25],   # æ ·æœ¬1ï¼šç±»åˆ«0çš„æ¦‚ç‡æ˜¯0.75ï¼Œç±»åˆ«1çš„æ¦‚ç‡æ˜¯0.25\n [0.20, 0.80],   # æ ·æœ¬2ï¼š...\n ...]\n \n[:, 0] è¡¨ç¤ºï¼šåªå–æ¯è¡Œçš„ç¬¬ä¸€ä¸ªå€¼ï¼ˆä¹Ÿå°±æ˜¯â€œå±äºç±»0â€çš„æ¦‚ç‡ï¼‰","metadata":{}},{"cell_type":"markdown","source":"# Training a model with hyperparameter tunning\n\nHyper-parameter tuning is enabled by specifying the tuner constructor argument of the model. The tuner object contains all the configuration of the tuner (search space, optimizer, trial and objective).\n","metadata":{}},{"cell_type":"code","source":"tuner = tfdf.tuner.RandomSearch(num_trials=1000)\ntuner.choice(\"min_examples\", [2, 5, 7, 10])\ntuner.choice(\"categorical_algorithm\", [\"CART\", \"RANDOM\"])\n\nlocal_search_space = tuner.choice(\"growing_strategy\", [\"LOCAL\"])\nlocal_search_space.choice(\"max_depth\", [3, 4, 5, 6, 8])\n\nglobal_search_space = tuner.choice(\"growing_strategy\", [\"BEST_FIRST_GLOBAL\"], merge=True)\nglobal_search_space.choice(\"max_num_nodes\", [16, 32, 64, 128, 256])\n\n#tuner.choice(\"use_hessian_gain\", [True, False])\ntuner.choice(\"shrinkage\", [0.02, 0.05, 0.10, 0.15])\ntuner.choice(\"num_candidate_attributes_ratio\", [0.2, 0.5, 0.9, 1.0])\n\n\ntuner.choice(\"split_axis\", [\"AXIS_ALIGNED\"])\noblique_space = tuner.choice(\"split_axis\", [\"SPARSE_OBLIQUE\"], merge=True)\noblique_space.choice(\"sparse_oblique_normalization\",\n                     [\"NONE\", \"STANDARD_DEVIATION\", \"MIN_MAX\"])\noblique_space.choice(\"sparse_oblique_weights\", [\"BINARY\", \"CONTINUOUS\"])\noblique_space.choice(\"sparse_oblique_num_projections_exponent\", [1.0, 1.5])\n\n# Tune the model. Notice the `tuner=tuner`.\ntuned_model = tfdf.keras.GradientBoostedTreesModel(tuner=tuner)\ntuned_model.fit(train_ds, verbose=0)\n\ntuned_self_evaluation = tuned_model.make_inspector().evaluation()\nprint(f\"Accuracy: {tuned_self_evaluation.accuracy} Loss:{tuned_self_evaluation.loss}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:23:13.249675Z","iopub.execute_input":"2023-04-17T15:23:13.251376Z","iopub.status.idle":"2023-04-17T15:25:19.611729Z","shell.execute_reply.started":"2023-04-17T15:23:13.251306Z","shell.execute_reply":"2023-04-17T15:25:19.610154Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸ¯ TF-DF GBDT æ¨¡å‹è‡ªåŠ¨è°ƒå‚ç¬”è®°ï¼ˆRandom Search å…¨æ³¨è§£ï¼‰\n\nä½¿ç”¨ TensorFlow Decision Forests çš„ `RandomSearch` ç­–ç•¥ï¼Œå¯¹ Gradient Boosted Trees æ¨¡å‹è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–ï¼Œä»¥æå‡é¢„æµ‹æ€§èƒ½ã€‚\n\n```python\n# åˆå§‹åŒ– Random Search Tunerï¼Œæœ€å¤šå°è¯• 1000 ä¸ªè¶…å‚æ•°ç»„åˆ\ntuner = tfdf.tuner.RandomSearch(num_trials=1000)\n\n# åŸºæœ¬å‚æ•°å®šä¹‰\ntuner.choice(\"min_examples\", [2, 5, 7, 10])  # æ¯ä¸ªå¶èŠ‚ç‚¹çš„æœ€å°æ ·æœ¬æ•°ï¼Œè¶Šå°æ¨¡å‹è¶Šå¤æ‚\ntuner.choice(\"categorical_algorithm\", [\"CART\", \"RANDOM\"])  # ç±»åˆ«ç‰¹å¾å¤„ç†æ–¹å¼ï¼šç©·ä¸¾ or éšæœº\ntuner.choice(\"shrinkage\", [0.02, 0.05, 0.10, 0.15])  # å­¦ä¹ ç‡\ntuner.choice(\"num_candidate_attributes_ratio\", [0.2, 0.5, 0.9, 1.0])  # åˆ†è£‚æ—¶ä½¿ç”¨çš„ç‰¹å¾æ¯”ä¾‹\n\n# æ ‘çš„ç”Ÿé•¿ç­–ç•¥ï¼šå±€éƒ¨ç­–ç•¥ï¼ˆLOCALï¼‰\nlocal_space = tuner.choice(\"growing_strategy\", [\"LOCAL\"])\nlocal_space.choice(\"max_depth\", [3, 4, 5, 6, 8])  # æ¯æ£µæ ‘çš„æœ€å¤§æ·±åº¦\n\n# æ ‘çš„ç”Ÿé•¿ç­–ç•¥ï¼šå…¨å±€ç­–ç•¥ï¼ˆBEST_FIRST_GLOBALï¼‰\nglobal_space = tuner.choice(\"growing_strategy\", [\"BEST_FIRST_GLOBAL\"], merge=True)\nglobal_space.choice(\"max_num_nodes\", [16, 32, 64, 128, 256])  # æ§åˆ¶æ ‘çš„æœ€å¤§èŠ‚ç‚¹æ•°\n\n# åˆ†è£‚æ–¹å¼ï¼šæ ‡å‡†å•è½´åˆ†è£‚\ntuner.choice(\"split_axis\", [\"AXIS_ALIGNED\"])\n\n# åˆ†è£‚æ–¹å¼ï¼šæ–œç‡åˆ†è£‚ï¼ˆç‰¹å¾çº¿æ€§ç»„åˆï¼‰ï¼Œæå‡è¡¨è¾¾èƒ½åŠ›\noblique_space = tuner.choice(\"split_axis\", [\"SPARSE_OBLIQUE\"], merge=True)\noblique_space.choice(\"sparse_oblique_normalization\", [\"NONE\", \"STANDARD_DEVIATION\", \"MIN_MAX\"])  # ç‰¹å¾å½’ä¸€åŒ–æ–¹å¼\noblique_space.choice(\"sparse_oblique_weights\", [\"BINARY\", \"CONTINUOUS\"])  # æƒé‡æ˜¯ 0/1 è¿˜æ˜¯è¿ç»­å€¼\noblique_space.choice(\"sparse_oblique_num_projections_exponent\", [1.0, 1.5])  # æ§åˆ¶ç»„åˆæ•°é‡ï¼ŒæŒ‡æ•°å½¢å¼\n\n# æ„å»ºæ¨¡å‹ï¼Œå¹¶ä¼ å…¥ tunerï¼ˆå¼€å§‹è‡ªåŠ¨è°ƒå‚ + è®­ç»ƒï¼‰\ntuned_model = tfdf.keras.GradientBoostedTreesModel(tuner=tuner)\ntuned_model.fit(train_ds, verbose=0)\n\n# è·å–æ¨¡å‹çš„è¯„ä¼°ç»“æœï¼ˆaccuracy å’Œ lossï¼‰\nresult = tuned_model.make_inspector().evaluation()\nprint(f\"Accuracy: {result.accuracy}  Loss: {result.loss}\")\n\n\nmin_examplesï¼šæ§åˆ¶è¿‡æ‹Ÿåˆ vs æ‹Ÿåˆèƒ½åŠ›\n\ncategorical_algorithmï¼šç±»åˆ«ç‰¹å¾åˆ†è£‚æ–¹å¼ï¼ŒRANDOM æ›´é€‚åˆé«˜åŸºæ•°\n\nshrinkageï¼šå­¦ä¹ ç‡ï¼Œè¶Šå°è¶Šç¨³å®šä½†è®­ç»ƒæ…¢\n\ngrowing_strategyï¼šå†³å®šæ ‘æ˜¯ç”¨â€œæœ€å¤§æ·±åº¦â€è¿˜æ˜¯â€œæœ€å¤§èŠ‚ç‚¹æ•°â€æ§åˆ¶ç”Ÿé•¿\n\nsplit_axisï¼šæ˜¯å¦ä½¿ç”¨ç¨€ç–æ–œç‡ï¼ˆç‰¹å¾ç»„åˆï¼‰è¿›è¡Œåˆ†è£‚ï¼Œæå‡æ¨¡å‹è¡¨è¾¾èƒ½åŠ›\n\næ‰€æœ‰ .choice(...) éƒ½ä¼šè¿›å…¥æœç´¢ç©ºé—´ï¼Œç”± RandomSearch è‡ªåŠ¨è¯•éªŒç»„åˆ","metadata":{}},{"cell_type":"markdown","source":"In the last line in the cell above, you can see the accuracy is higher than previously with default parameters and parameters set by hand.\n\nThis is the main idea behing hyperparameter tuning.\n\nFor more information you can follow this tutorial: [Automated hyper-parameter tuning](https://www.tensorflow.org/decision_forests/tutorials/automatic_tuning_colab)","metadata":{}},{"cell_type":"markdown","source":"# Making an ensemble\n\nHere you'll create 100 models with different seeds and combine their results\n\nThis approach removes a little bit the random aspects related to creating ML models\n\nIn the GBT creation is used the `honest` parameter. It will use different training examples to infer the structure and the leaf values. This regularization technique trades examples for bias estimates.","metadata":{}},{"cell_type":"markdown","source":"ç”¨äº ensemble çš„æ¨¡å‹ä¸€èˆ¬æ›´åå¥½â€œå°‘è°ƒå‚ + å¤šæ ·æ€§â€ã€‚","metadata":{}},{"cell_type":"code","source":"predictions = None\nnum_predictions = 0\n\nfor i in range(100):\n    print(f\"i:{i}\")\n    # Possible models: GradientBoostedTreesModel or RandomForestModel\n    model = tfdf.keras.GradientBoostedTreesModel(\n        verbose=0, # Very few logs\n        features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n        exclude_non_specified_features=True, # Only use the features in \"features\"\n\n        #min_examples=1,\n        #categorical_algorithm=\"RANDOM\",\n        ##max_depth=4,\n        #shrinkage=0.05,\n        ##num_candidate_attributes_ratio=0.2,\n        #split_axis=\"SPARSE_OBLIQUE\",\n        #sparse_oblique_normalization=\"MIN_MAX\",\n        #sparse_oblique_num_projections_exponent=2.0,\n        #num_trees=2000,\n        ##validation_ratio=0.0,\n        random_seed=i,\n        honest=True,\n    )\n    model.fit(train_ds)\n    \n    sub_predictions = model.predict(serving_ds, verbose=0)[:,0]\n    if predictions is None:\n        predictions = sub_predictions\n    else:\n        predictions += sub_predictions\n    num_predictions += 1\n\npredictions/=num_predictions\n\nkaggle_predictions = pd.DataFrame({\n        \"PassengerId\": serving_df[\"PassengerId\"],\n        \"Survived\": (predictions >= 0.5).astype(int)\n    })\n\nmake_submission(kaggle_predictions)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:28:37.557745Z","iopub.execute_input":"2023-04-17T15:28:37.558172Z","iopub.status.idle":"2023-04-17T15:28:52.809698Z","shell.execute_reply.started":"2023-04-17T15:28:37.55813Z","shell.execute_reply":"2023-04-17T15:28:52.808193Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ä»€ä¹ˆæ˜¯è¯šå®åˆ†è£‚ï¼ˆHonest Treesï¼‰ï¼Ÿ\n æ ‡å‡†åˆ†è£‚ï¼ˆéè¯šå®ï¼‰ï¼š\næ‰€æœ‰æ ·æœ¬æ—¢ç”¨æ¥å†³å®šå¦‚ä½•åˆ†è£‚èŠ‚ç‚¹ï¼ˆå»ºæ ‘ï¼‰ï¼Œä¹Ÿç”¨æ¥è®¡ç®—æ¯ä¸ªå¶èŠ‚ç‚¹çš„é¢„æµ‹å€¼ã€‚\n\n è¯šå®åˆ†è£‚ï¼ˆhonest=Trueï¼‰ï¼š\næ ·æœ¬ä¼šè¢«éšæœºæ‹†åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š\n\nç»“æ„é›†ï¼ˆstructure setï¼‰ï¼šç”¨äºå†³å®šå¦‚ä½•åˆ†è£‚\n\nä¼°è®¡é›†ï¼ˆestimation setï¼‰ï¼šç”¨äºè®¡ç®—å¶èŠ‚ç‚¹è¾“å‡ºå€¼ï¼ˆå¦‚ç±»åˆ«æ¦‚ç‡ï¼‰\n\n### ä¸ºä»€ä¹ˆè¦è¿™ä¹ˆåšï¼Ÿ\nä¼˜ç‚¹ï¼š\né¿å…ä¿¡æ¯æ³„éœ²ï¼šå»ºæ ‘ç”¨çš„æ•°æ®ä¸å†å‚ä¸é¢„æµ‹ä¼°è®¡ï¼Œå‡å°‘è¿‡æ‹Ÿåˆã€‚\n\næ›´ç¨³å¥çš„é¢„æµ‹å€¼ï¼šå› ä¸ºé¢„æµ‹å€¼ä¸æ˜¯å¯¹è®­ç»ƒé›†çš„â€œå›å£°æ‹Ÿåˆâ€ï¼Œè€Œæ˜¯ç‹¬ç«‹ä¼°è®¡ã€‚\n\nåœ¨é›†æˆï¼ˆå¤šä¸ªæ ‘ï¼‰æ—¶ç‰¹åˆ«æœ‰æ•ˆï¼šæ¯æ£µæ ‘éƒ½æ›´â€œè°¦é€Šâ€ï¼Œåˆèµ·æ¥æ³›åŒ–èƒ½åŠ›å¼ºã€‚","metadata":{}},{"cell_type":"markdown","source":"# What is next\n\nIf you want to learn more about TensorFlow Decision Forests and its advanced features, you can follow the official documentation [here](https://www.tensorflow.org/decision_forests) ","metadata":{}}]}