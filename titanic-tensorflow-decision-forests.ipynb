{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30458,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic competition with TensorFlow Decision Forests\n\nThis notebook will take you through the steps needed to train a baseline Gradient Boosted Trees Model using TensorFlow Decision Forests and creating a submission on the Titanic competition. \n\nThis notebook shows:\n\n1. How to do some basic pre-processing. For example, the passenger names will be tokenized, and ticket names will be splitted in parts.\n1. How to train a Gradient Boosted Trees (GBT) with default parameters\n1. How to train a GBT with improved default parameters\n1. How to tune the parameters of a GBTs\n1. How to train and ensemble many GBTs","metadata":{}},{"cell_type":"markdown","source":"Gradient Boosted Trees（梯度提升树，简称GBT或GBDT）是一种集成学习方法，主要用于回归和分类任务。它通过**集成多个弱学习器（通常是决策树）**来形成一个强学习器。这里是一个信息密度高的解释：\n\n核心思想\n迭代训练： 每一轮训练一个新树，用来纠正之前模型的残差（即预测误差）。\n\n加权求和： 所有树的预测结果加权相加，得到最终预测值。\n\n梯度下降： 利用损失函数的梯度，指导每棵新树如何拟合残差。","metadata":{}},{"cell_type":"markdown","source":"# Imports dependencies","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nimport tensorflow as tf\nimport tensorflow_decision_forests as tfdf\n\nprint(f\"Found TF-DF {tfdf.__version__}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:32:04.264037Z","iopub.execute_input":"2023-04-17T15:32:04.26451Z","iopub.status.idle":"2023-04-17T15:32:04.272355Z","shell.execute_reply.started":"2023-04-17T15:32:04.264459Z","shell.execute_reply":"2023-04-17T15:32:04.270819Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load dataset","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\nserving_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\ntrain_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:30:04.102889Z","iopub.execute_input":"2023-04-17T15:30:04.10358Z","iopub.status.idle":"2023-04-17T15:30:04.158126Z","shell.execute_reply.started":"2023-04-17T15:30:04.103539Z","shell.execute_reply":"2023-04-17T15:30:04.156877Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare dataset\n\nWe will apply the following transformations on the dataset.\n\n1. Tokenize the names. For example, \"Braund, Mr. Owen Harris\" will become [\"Braund\", \"Mr.\", \"Owen\", \"Harris\"].\n2. Extract any prefix in the ticket. For example ticket \"STON/O2. 3101282\" will become \"STON/O2.\" and 3101282.","metadata":{}},{"cell_type":"code","source":"def preprocess(df):\n    df = df.copy()\n    \n    def normalize_name(x):\n        return \" \".join([v.strip(\",()[].\\\"'\") for v in x.split(\" \")])\n    \n    def ticket_number(x):\n        return x.split(\" \")[-1]\n        \n    def ticket_item(x):\n        items = x.split(\" \")\n        if len(items) == 1:\n            return \"NONE\"\n        return \"_\".join(items[0:-1])\n    \n    df[\"Name\"] = df[\"Name\"].apply(normalize_name)\n    df[\"Ticket_number\"] = df[\"Ticket\"].apply(ticket_number)\n    df[\"Ticket_item\"] = df[\"Ticket\"].apply(ticket_item)                     \n    return df\n    \npreprocessed_train_df = preprocess(train_df)\npreprocessed_serving_df = preprocess(serving_df)\n\npreprocessed_train_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:30:09.50201Z","iopub.execute_input":"2023-04-17T15:30:09.502467Z","iopub.status.idle":"2023-04-17T15:30:09.540151Z","shell.execute_reply.started":"2023-04-17T15:30:09.502432Z","shell.execute_reply":"2023-04-17T15:30:09.538948Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's keep the list of the input features of the model. Notably, we don't want to train our model on the \"PassengerId\" and \"Ticket\" features.","metadata":{}},{"cell_type":"code","source":"input_features = list(preprocessed_train_df.columns)\ninput_features.remove(\"Ticket\")\ninput_features.remove(\"PassengerId\")\ninput_features.remove(\"Survived\")\n#input_features.remove(\"Ticket_number\")\n\nprint(f\"Input features: {input_features}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:30:14.000466Z","iopub.execute_input":"2023-04-17T15:30:14.000868Z","iopub.status.idle":"2023-04-17T15:30:14.00835Z","shell.execute_reply.started":"2023-04-17T15:30:14.000833Z","shell.execute_reply":"2023-04-17T15:30:14.006982Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Convert Pandas dataset to TensorFlow Dataset","metadata":{}},{"cell_type":"code","source":"def tokenize_names(features, labels=None):\n    \"\"\"Divite the names into tokens. TF-DF can consume text tokens natively.\"\"\"\n    features[\"Name\"] =  tf.strings.split(features[\"Name\"])\n    return features, labels\n\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_train_df,label=\"Survived\").map(tokenize_names)\nserving_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_serving_df).map(tokenize_names)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:30:17.896317Z","iopub.execute_input":"2023-04-17T15:30:17.896741Z","iopub.status.idle":"2023-04-17T15:30:18.231195Z","shell.execute_reply.started":"2023-04-17T15:30:17.896705Z","shell.execute_reply":"2023-04-17T15:30:18.229792Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"features 是一个字典，键是列名，值是张量。\n\ntf.strings.split(features[\"Name\"]) 会将姓名字符串按空格拆分成列表（词 tokens），如 \"Smith, John\" -> [\"Smith,\", \"John\"]\n\nTF-DF 可以直接使用 token 列作为文本特征，不像一般的模型需要做 embedding。\n\n最后返回修改后的 features 和标签 labels（如果有）。\n\npd_dataframe_to_tf_dataset(...)：把 pandas 的 DataFrame 转成 TensorFlow 的数据集。\n\nlabel=\"Survived\"：指定目标变量。\n\n.map(tokenize_names)：对数据集的每一条记录应用 tokenize_names 函数，即进行姓名 token 化。","metadata":{}},{"cell_type":"markdown","source":"# Train model with default parameters\n\n### Train model\n\nFirst, we are training a GradientBoostedTreesModel model with the default parameters.","metadata":{}},{"cell_type":"code","source":"model = tfdf.keras.GradientBoostedTreesModel(\n    verbose=0, # Very few logs\n    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n    exclude_non_specified_features=True, # Only use the features in \"features\"\n    random_seed=1234,\n)\nmodel.fit(train_ds)\n\nself_evaluation = model.make_inspector().evaluation()\nprint(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:32:16.580089Z","iopub.execute_input":"2023-04-17T15:32:16.581306Z","iopub.status.idle":"2023-04-17T15:32:17.55132Z","shell.execute_reply.started":"2023-04-17T15:32:16.581257Z","shell.execute_reply":"2023-04-17T15:32:17.55024Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train model with improved default parameters\n\nNow you'll use some specific parameters when creating the GBT model","metadata":{}},{"cell_type":"code","source":"model = tfdf.keras.GradientBoostedTreesModel(\n    verbose=0, # Very few logs\n    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n    exclude_non_specified_features=True, # Only use the features in \"features\"\n    \n    #num_trees=2000,\n    \n    # Only for GBT.\n    # A bit slower, but great to understand the model.\n    # compute_permutation_variable_importance=True,\n    \n    # Change the default hyper-parameters\n    # hyperparameter_template=\"benchmark_rank1@v1\",\n    \n    #num_trees=1000,\n    #tuner=tuner\n    \n    min_examples=1,\n    categorical_algorithm=\"RANDOM\",\n    #max_depth=4,\n    shrinkage=0.05,\n    #num_candidate_attributes_ratio=0.2,\n    split_axis=\"SPARSE_OBLIQUE\",\n    sparse_oblique_normalization=\"MIN_MAX\",\n    sparse_oblique_num_projections_exponent=2.0,\n    num_trees=2000,\n    #validation_ratio=0.0,\n    random_seed=1234,\n    \n)\nmodel.fit(train_ds)\n\nself_evaluation = model.make_inspector().evaluation()\nprint(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:06:14.822939Z","iopub.execute_input":"2023-04-17T15:06:14.82343Z","iopub.status.idle":"2023-04-17T15:06:16.103565Z","shell.execute_reply.started":"2023-04-17T15:06:14.82339Z","shell.execute_reply":"2023-04-17T15:06:16.102272Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 超参数（重点部分）\nmin_examples=1\n每个叶节点最少样本数，设置为 1 意味着允许很深的树（可能过拟合，但适合学习复杂模式）。\n\ncategorical_algorithm=\"RANDOM\"\n分类变量分裂策略为随机选择（通常用于大类变量）。\n\nshrinkage=0.05\n学习率（又名 shrinkage），控制每棵树对最终预测的贡献，越小越稳定但训练慢。\n\nsplit_axis=\"SPARSE_OBLIQUE\"\n使用稀疏斜率分裂轴（Sparse Oblique Split）：允许用多个特征的线性组合进行分裂，而非一个特征一个分裂（更复杂的模型能力）。\n\nsparse_oblique_normalization=\"MIN_MAX\"\n对线性组合中的特征进行 Min-Max 归一化。\n\nsparse_oblique_num_projections_exponent=2.0\n控制生成多少种组合分裂方式。这个值是指数，实际是 \\text{num_features}^{2.0} 个组合。\n\nnum_trees=2000\n训练 2000 棵树，模型越复杂，拟合能力越强。\n\nrandom_seed=1234\n固定随机种子，使结果可复现。","metadata":{}},{"cell_type":"markdown","source":"### 1. categorical_algorithm=\"RANDOM\"\n这是指定类别特征如何分裂（即决策树中如何处理字符串类型变量）的方式之一。TF-DF 支持几种算法，\"RANDOM\" 是其中较特殊的一种。\n\n作用：\n对于某个类别特征，树在选择分裂点时不会枚举所有可能的子集，而是随机挑选一些子集进行评估。\n\n为什么这样做？\n枚举所有子集代价非常高：如果一个特征有 20 个类别，理论上有 \n2^19-1=524,287 种可能的分裂方式。\n\n使用 \"RANDOM\" 算法可显著减少计算量，尤其在类别数量多的时候。\n\n适用场景：\n特征类别数多（例如名字、地名）\n\n数据量大，训练速度重要\n\n不追求精确最优分裂点，只求近似好结果","metadata":{}},{"cell_type":"markdown","source":"### 2. sparse_oblique_normalization=\"MIN_MAX\"\n这个参数是在使用 \"SPARSE_OBLIQUE\" 分裂轴时，指定线性组合中用到的数值特征如何归一化。\n\n背景：SPARSE_OBLIQUE\n稀疏斜率（sparse oblique）是对标准 axis-aligned 决策树的增强，它允许用多个特征的线性组合进行决策\n\n多个特征的线性组合用于节点分裂：\n\n$w_1 x_1 + w_2 x_2 + \\cdots + w_k x_k < \\text{threshold}$\n\n\n其中：\n- \\( w_i \\)：每个特征的权重\n- \\( x_i \\)：特征值\n- \\( k \\)：特征数量\n\n\n为了让这些组合更稳定，需要对特征做标准化。\n\n\"MIN_MAX\" 的含义：\n每个特征会按如下方式缩放：\n使用最小-最大归一化对特征 \\( x \\) 做缩放：\n\n$$\nx' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n$$\n\n归一化后有：\n\n$$\nx' \\in [0, 1]\n$$\n\n\n这样所有特征的取值都被归一化到 [0, 1] 范围内。\n\n目的：\n让不同尺度的特征在组合时权重更加可比\n\n避免某些取值范围大的特征主导分裂方向\n\n可选项：\n还有比如 \"Z_SCORE\"（标准差归一）等方式，但 MIN_MAX 更直观且适用于多数场景。\n\n总结一句话：\ncategorical_algorithm=\"RANDOM\" 是让高基数类别变量分裂更高效；\n\nsparse_oblique_normalization=\"MIN_MAX\" 是保证斜率分裂中多特征组合的尺度统一性。","metadata":{}},{"cell_type":"markdown","source":"Let's look at the model and you can also notice the information about variable importance that the model figured out","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:06:16.104878Z","iopub.execute_input":"2023-04-17T15:06:16.105239Z","iopub.status.idle":"2023-04-17T15:06:16.123439Z","shell.execute_reply.started":"2023-04-17T15:06:16.105206Z","shell.execute_reply":"2023-04-17T15:06:16.121993Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"model.summary() 是 TensorFlow / Keras 中常用的方法之一，作用是输出模型结构摘要。不过，在你使用的 TensorFlow Decision Forests (TF-DF) 中，它的作用稍有不同。\n\n在 TF-DF 中 model.summary() 的作用：\n它会打印出一个简略的模型结构信息，包括：\n\n模型类型（如 GradientBoostedTreesModel）\n\n树的数量、深度、节点数等\n\n特征使用情况（例如用了哪些特征）\n\n一些超参数设置\n\n","metadata":{}},{"cell_type":"markdown","source":"# Make predictions","metadata":{}},{"cell_type":"code","source":"def prediction_to_kaggle_format(model, threshold=0.5):\n    proba_survive = model.predict(serving_ds, verbose=0)[:,0]\n    return pd.DataFrame({\n        \"PassengerId\": serving_df[\"PassengerId\"],\n        \"Survived\": (proba_survive >= threshold).astype(int)\n    })\n\ndef make_submission(kaggle_predictions):\n    path=\"/kaggle/working/submission.csv\"\n    kaggle_predictions.to_csv(path, index=False)\n    print(f\"Submission exported to {path}\")\n    \nkaggle_predictions = prediction_to_kaggle_format(model)\nmake_submission(kaggle_predictions)\n!head /kaggle/working/submission.csv","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:06:16.126575Z","iopub.execute_input":"2023-04-17T15:06:16.126941Z","iopub.status.idle":"2023-04-17T15:06:17.406876Z","shell.execute_reply.started":"2023-04-17T15:06:16.126904Z","shell.execute_reply":"2023-04-17T15:06:17.405022Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### model.predict(serving_ds, verbose=0)\nserving_ds:\n一个 TensorFlow Dataset，不含标签，用于预测。一般是测试集或部署用数据。\n\nverbose=0:\n设置日志等级为“静默”，不输出任何进度条或预测日志。常用于脚本执行或 notebook 干净输出。\n\n[:, 0] 的作用\n.predict(...) 返回的是一个 NumPy 数组，形状大致为 (num_samples, num_classes)。\n\n对于二分类问题（如 Titanic 是否生存），它返回的是每个样本的两个概率：\n\n[[0.75, 0.25],   # 样本1：类别0的概率是0.75，类别1的概率是0.25\n [0.20, 0.80],   # 样本2：...\n ...]\n \n[:, 0] 表示：只取每行的第一个值（也就是“属于类0”的概率）","metadata":{}},{"cell_type":"markdown","source":"# Training a model with hyperparameter tunning\n\nHyper-parameter tuning is enabled by specifying the tuner constructor argument of the model. The tuner object contains all the configuration of the tuner (search space, optimizer, trial and objective).\n","metadata":{}},{"cell_type":"code","source":"tuner = tfdf.tuner.RandomSearch(num_trials=1000)\ntuner.choice(\"min_examples\", [2, 5, 7, 10])\ntuner.choice(\"categorical_algorithm\", [\"CART\", \"RANDOM\"])\n\nlocal_search_space = tuner.choice(\"growing_strategy\", [\"LOCAL\"])\nlocal_search_space.choice(\"max_depth\", [3, 4, 5, 6, 8])\n\nglobal_search_space = tuner.choice(\"growing_strategy\", [\"BEST_FIRST_GLOBAL\"], merge=True)\nglobal_search_space.choice(\"max_num_nodes\", [16, 32, 64, 128, 256])\n\n#tuner.choice(\"use_hessian_gain\", [True, False])\ntuner.choice(\"shrinkage\", [0.02, 0.05, 0.10, 0.15])\ntuner.choice(\"num_candidate_attributes_ratio\", [0.2, 0.5, 0.9, 1.0])\n\n\ntuner.choice(\"split_axis\", [\"AXIS_ALIGNED\"])\noblique_space = tuner.choice(\"split_axis\", [\"SPARSE_OBLIQUE\"], merge=True)\noblique_space.choice(\"sparse_oblique_normalization\",\n                     [\"NONE\", \"STANDARD_DEVIATION\", \"MIN_MAX\"])\noblique_space.choice(\"sparse_oblique_weights\", [\"BINARY\", \"CONTINUOUS\"])\noblique_space.choice(\"sparse_oblique_num_projections_exponent\", [1.0, 1.5])\n\n# Tune the model. Notice the `tuner=tuner`.\ntuned_model = tfdf.keras.GradientBoostedTreesModel(tuner=tuner)\ntuned_model.fit(train_ds, verbose=0)\n\ntuned_self_evaluation = tuned_model.make_inspector().evaluation()\nprint(f\"Accuracy: {tuned_self_evaluation.accuracy} Loss:{tuned_self_evaluation.loss}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:23:13.249675Z","iopub.execute_input":"2023-04-17T15:23:13.251376Z","iopub.status.idle":"2023-04-17T15:25:19.611729Z","shell.execute_reply.started":"2023-04-17T15:23:13.251306Z","shell.execute_reply":"2023-04-17T15:25:19.610154Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 🎯 TF-DF GBDT 模型自动调参笔记（Random Search 全注解）\n\n使用 TensorFlow Decision Forests 的 `RandomSearch` 策略，对 Gradient Boosted Trees 模型进行超参数优化，以提升预测性能。\n\n```python\n# 初始化 Random Search Tuner，最多尝试 1000 个超参数组合\ntuner = tfdf.tuner.RandomSearch(num_trials=1000)\n\n# 基本参数定义\ntuner.choice(\"min_examples\", [2, 5, 7, 10])  # 每个叶节点的最小样本数，越小模型越复杂\ntuner.choice(\"categorical_algorithm\", [\"CART\", \"RANDOM\"])  # 类别特征处理方式：穷举 or 随机\ntuner.choice(\"shrinkage\", [0.02, 0.05, 0.10, 0.15])  # 学习率\ntuner.choice(\"num_candidate_attributes_ratio\", [0.2, 0.5, 0.9, 1.0])  # 分裂时使用的特征比例\n\n# 树的生长策略：局部策略（LOCAL）\nlocal_space = tuner.choice(\"growing_strategy\", [\"LOCAL\"])\nlocal_space.choice(\"max_depth\", [3, 4, 5, 6, 8])  # 每棵树的最大深度\n\n# 树的生长策略：全局策略（BEST_FIRST_GLOBAL）\nglobal_space = tuner.choice(\"growing_strategy\", [\"BEST_FIRST_GLOBAL\"], merge=True)\nglobal_space.choice(\"max_num_nodes\", [16, 32, 64, 128, 256])  # 控制树的最大节点数\n\n# 分裂方式：标准单轴分裂\ntuner.choice(\"split_axis\", [\"AXIS_ALIGNED\"])\n\n# 分裂方式：斜率分裂（特征线性组合），提升表达能力\noblique_space = tuner.choice(\"split_axis\", [\"SPARSE_OBLIQUE\"], merge=True)\noblique_space.choice(\"sparse_oblique_normalization\", [\"NONE\", \"STANDARD_DEVIATION\", \"MIN_MAX\"])  # 特征归一化方式\noblique_space.choice(\"sparse_oblique_weights\", [\"BINARY\", \"CONTINUOUS\"])  # 权重是 0/1 还是连续值\noblique_space.choice(\"sparse_oblique_num_projections_exponent\", [1.0, 1.5])  # 控制组合数量，指数形式\n\n# 构建模型，并传入 tuner（开始自动调参 + 训练）\ntuned_model = tfdf.keras.GradientBoostedTreesModel(tuner=tuner)\ntuned_model.fit(train_ds, verbose=0)\n\n# 获取模型的评估结果（accuracy 和 loss）\nresult = tuned_model.make_inspector().evaluation()\nprint(f\"Accuracy: {result.accuracy}  Loss: {result.loss}\")\n\n\nmin_examples：控制过拟合 vs 拟合能力\n\ncategorical_algorithm：类别特征分裂方式，RANDOM 更适合高基数\n\nshrinkage：学习率，越小越稳定但训练慢\n\ngrowing_strategy：决定树是用“最大深度”还是“最大节点数”控制生长\n\nsplit_axis：是否使用稀疏斜率（特征组合）进行分裂，提升模型表达能力\n\n所有 .choice(...) 都会进入搜索空间，由 RandomSearch 自动试验组合","metadata":{}},{"cell_type":"markdown","source":"In the last line in the cell above, you can see the accuracy is higher than previously with default parameters and parameters set by hand.\n\nThis is the main idea behing hyperparameter tuning.\n\nFor more information you can follow this tutorial: [Automated hyper-parameter tuning](https://www.tensorflow.org/decision_forests/tutorials/automatic_tuning_colab)","metadata":{}},{"cell_type":"markdown","source":"# Making an ensemble\n\nHere you'll create 100 models with different seeds and combine their results\n\nThis approach removes a little bit the random aspects related to creating ML models\n\nIn the GBT creation is used the `honest` parameter. It will use different training examples to infer the structure and the leaf values. This regularization technique trades examples for bias estimates.","metadata":{}},{"cell_type":"markdown","source":"用于 ensemble 的模型一般更偏好“少调参 + 多样性”。","metadata":{}},{"cell_type":"code","source":"predictions = None\nnum_predictions = 0\n\nfor i in range(100):\n    print(f\"i:{i}\")\n    # Possible models: GradientBoostedTreesModel or RandomForestModel\n    model = tfdf.keras.GradientBoostedTreesModel(\n        verbose=0, # Very few logs\n        features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n        exclude_non_specified_features=True, # Only use the features in \"features\"\n\n        #min_examples=1,\n        #categorical_algorithm=\"RANDOM\",\n        ##max_depth=4,\n        #shrinkage=0.05,\n        ##num_candidate_attributes_ratio=0.2,\n        #split_axis=\"SPARSE_OBLIQUE\",\n        #sparse_oblique_normalization=\"MIN_MAX\",\n        #sparse_oblique_num_projections_exponent=2.0,\n        #num_trees=2000,\n        ##validation_ratio=0.0,\n        random_seed=i,\n        honest=True,\n    )\n    model.fit(train_ds)\n    \n    sub_predictions = model.predict(serving_ds, verbose=0)[:,0]\n    if predictions is None:\n        predictions = sub_predictions\n    else:\n        predictions += sub_predictions\n    num_predictions += 1\n\npredictions/=num_predictions\n\nkaggle_predictions = pd.DataFrame({\n        \"PassengerId\": serving_df[\"PassengerId\"],\n        \"Survived\": (predictions >= 0.5).astype(int)\n    })\n\nmake_submission(kaggle_predictions)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:28:37.557745Z","iopub.execute_input":"2023-04-17T15:28:37.558172Z","iopub.status.idle":"2023-04-17T15:28:52.809698Z","shell.execute_reply.started":"2023-04-17T15:28:37.55813Z","shell.execute_reply":"2023-04-17T15:28:52.808193Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 什么是诚实分裂（Honest Trees）？\n 标准分裂（非诚实）：\n所有样本既用来决定如何分裂节点（建树），也用来计算每个叶节点的预测值。\n\n 诚实分裂（honest=True）：\n样本会被随机拆分为两部分：\n\n结构集（structure set）：用于决定如何分裂\n\n估计集（estimation set）：用于计算叶节点输出值（如类别概率）\n\n### 为什么要这么做？\n优点：\n避免信息泄露：建树用的数据不再参与预测估计，减少过拟合。\n\n更稳健的预测值：因为预测值不是对训练集的“回声拟合”，而是独立估计。\n\n在集成（多个树）时特别有效：每棵树都更“谦逊”，合起来泛化能力强。","metadata":{}},{"cell_type":"markdown","source":"# What is next\n\nIf you want to learn more about TensorFlow Decision Forests and its advanced features, you can follow the official documentation [here](https://www.tensorflow.org/decision_forests) ","metadata":{}}]}