**信息论（Information Theory）** 是研究信息的度量、传输、存储和压缩的一门科学，由克劳德·香农（Claude Shannon）在 1948 年提出的奠基论文中首次系统化。它是现代通信、数据压缩和机器学习的重要理论基础。

---

### **1. 核心概念**

#### **1.1 信息量（Self-Information）**
- 定义：事件 \( x \) 的信息量 \( I(x) \) 表示事件发生时获取的信息大小。
- 数学公式：
  \[
  I(x) = -\log P(x)
  \]
  - \(P(x)\)：事件 \(x\) 的概率。
  - \(-\log\)：概率越小，信息量越大。

- 直观解释：
  - 罕见事件包含更多信息。
  - 例子：抛硬币 \(P(\text{正面}) = 0.5\)，信息量 \(I = -\log_2(0.5) = 1\) 比特。

---

#### **1.2 熵（Entropy）**
- 定义：熵 \(H(X)\) 是离散随机变量 \(X\) 的期望信息量，表示不确定性或信息平均量。
- 数学公式：
  \[
  H(X) = -\sum_{x \in X} P(x) \log P(x)
  \]
  - 单位：比特（以 2 为底）或纳特（以 \(e\) 为底）。

- 直观解释：
  - 熵越大，不确定性越高。
  - 均匀分布（每个事件的概率相等）具有最大熵。

---

#### **1.3 相对熵（KL 散度）**
- 定义：相对熵（KL 散度）衡量两个分布 \(P\) 和 \(Q\) 的相似程度。
- 数学公式：
  \[
  D_{\text{KL}}(P || Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}
  \]

- 直观解释：
  - \(D_{\text{KL}}\) 越小，分布 \(P\) 和 \(Q\) 越接近。
  - 它是非对称的，即 \(D_{\text{KL}}(P || Q) \neq D_{\text{KL}}(Q || P)\)。

---

#### **1.4 交叉熵（Cross-Entropy）**
- 定义：交叉熵是两个分布之间的信息差异，用于度量预测分布 \(Q(x)\) 和真实分布 \(P(x)\) 的不匹配程度。
- 数学公式：
  \[
  H(P, Q) = -\sum_x P(x) \log Q(x)
  \]

- 应用：
  - 在机器学习中，交叉熵是分类任务中的常用损失函数。

---

#### **1.5 信息增益（Information Gain, IG）**
- 定义：信息增益表示某个属性（或特征）对目标变量的不确定性减少的程度。
- 数学公式：
  \[
  IG(T, A) = H(T) - H(T|A)
  \]
  - \(H(T)\)：原始熵。
  - \(H(T|A)\)：在属性 \(A\) 已知的条件下的熵。

- 应用：
  - 决策树中，用于选择最佳分割特征。

---

### **2. 信息论在机器学习中的应用**

#### **2.1 交叉熵损失**
- 用于分类任务中，衡量预测分布与真实分布的差异。

#### **2.2 KL 散度**
- 在生成对抗网络（GAN）中衡量生成模型分布与真实分布的差异。
- 在变分自编码器（VAE）中，用于正则化潜在分布。

#### **2.3 信息增益**
- 决策树（如 ID3、C4.5）的特征选择标准。

#### **2.4 熵和数据压缩**
- Shannon 熵是理论上的数据压缩极限。
- 哈夫曼编码（Huffman Coding）实现了接近熵的压缩效率。

---

### **3. 示例代码**

#### **3.1 熵计算**
```python
import numpy as np

# 概率分布
P = np.array([0.2, 0.5, 0.3])

# 计算熵
H = -np.sum(P * np.log2(P))
print(f"Entropy: {H} bits")
```

#### **3.2 KL 散度计算**
```python
# 两个分布
P = np.array([0.1, 0.4, 0.5])
Q = np.array([0.2, 0.3, 0.5])

# KL 散度
D_KL = np.sum(P * np.log2(P / Q))
print(f"KL Divergence: {D_KL} bits")
```

#### **3.3 交叉熵计算**
```python
# 真实分布和预测分布
P = np.array([1, 0, 0])  # One-hot 编码
Q = np.array([0.7, 0.2, 0.1])

# 交叉熵
H_PQ = -np.sum(P * np.log2(Q))
print(f"Cross-Entropy: {H_PQ} bits")
```

---

### **4. 信息论的意义**
1. **度量不确定性**：熵衡量系统的平均不确定性。
2. **优化模型**：通过最大化信息增益或最小化交叉熵，提高模型性能。
3. **压缩与通信**：信息论为数据压缩和通信提供理论基础。

