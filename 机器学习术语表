Fastbook 第一章到第五章 简洁术语表

---

     **第一章：什么是机器学习？**
- **机器学习 (Machine Learning)**：从数据中学习模式以做出预测。
- **监督学习 (Supervised Learning)**：训练数据有目标标签。
- **无监督学习 (Unsupervised Learning)**：训练数据无目标标签。
- **训练集 (Training Set)**：用于训练模型的数据。
- **验证集 (Validation Set)**：用于评估模型性能的数据。

---

     **第二章：从模型到部署**
- **模型 (Model)**：数学函数，用于预测。
- **参数 (Parameters)**：模型训练中学习的权重。
- **损失函数 (Loss Function)**：衡量预测与真实值的差异。
  - 示例：均方误差 (MSE)。
- **优化器 (Optimizer)**：调整参数以最小化损失。
  - 示例：随机梯度下降 (SGD)。
- **学习率 (Learning Rate)**：控制参数调整的步幅大小。

---

     **第三章：第一个模型**
- **线性模型 (Linear Model)**：输入的线性组合作为输出。
  - 公式：\(y = wx + b\)。
- **回归 (Regression)**：预测连续值。
- **分类 (Classification)**：预测离散类别。
- **激活函数 (Activation Function)**：引入非线性，常用 ReLU 或 Sigmoid。

---

     **第四章：梯度下降**
- **梯度下降 (Gradient Descent)**：通过梯度调整参数以最小化损失。
- **梯度 (Gradient)**：损失函数对参数的导数。
- **学习率调度 (Learning Rate Scheduler)**：动态调整学习率。
- **过拟合 (Overfitting)**：模型在训练集表现好但验证集表现差。
- **欠拟合 (Underfitting)**：模型在训练集和验证集表现都不好。
- **反向传播 (Backpropagation)**：通过反向传播误差计算梯度。

---

     **第五章：更复杂的模型**
- **交叉熵损失 (Cross-Entropy Loss)**：分类任务中衡量预测分布与真实分布差异的损失函数。
- **Softmax**：将 logits 转换为概率分布。
- **验证损失 (Validation Loss)**：验证集上的损失。
- **迁移学习 (Transfer Learning)**：将预训练模型的知识迁移到新任务。
- **Fine-Tuning**：对预训练模型进行微调。
- **最大似然估计 (MLE)**：通过最大化数据的似然函数估计模型参数。

---

     **补充说明**
- **Batch Size**：每次训练中使用的数据样本数。
- **Epoch**：遍历整个训练集一次。
- **关键联系**：
  - 激活函数使模型具有非线性。
  - 梯度下降依赖损失函数和梯度。
  - 学习率调度器提高训练效率。




#### 第六章主要术语

1. **Transfer Learning (迁移学习)**

   - 定义：在一个预训练模型上进行屈合，使用远距任务上的矩积。
   - 应用：在图像分类中，将预训练模型用于新数据集。

2. **Fine-Tuning (微调)**

   - 定义：重新训练预训练模型的高层，以适配新的任务。
   - 过程：减少预训练高层的学习率，适应新数据。

3. **Discriminative Fine-Tuning (区分微调)**

   - 定义：对模型各层采用不同学习率进行微调。
   - 优势：保持低层特征，增强高层对新任务的适应性。

4. **Learning Rate Finder (学习率查找器)**

   - 定义：查找学习率之间最适合的范围，提升模型训练效率和结果。
   - 过程：训练模型，通过查看两边像比之间的每一步。

5. **Layer Freezing (层冻结)**

   - 定义：保持预训练模型某些层模型参数不变，只训练举固层。
   - 应用：提升训练速度，保持高比量上的评分。




第七章：迁移学习（Transfer Learning）
核心概念：

迁移学习：利用在大数据集（如 ImageNet）上预训练的模型，加速和优化新任务的学习。

FastAI 的 cnn_learner() 可快速加载预训练模型并开始训练。

主要步骤：

使用 cnn_learner() 创建模型：
learn = cnn_learner(dls, resnet34, metrics=accuracy)

冻结模型，只训练最后几层：
learn.fine_tune(epochs)

解冻模型，微调全部层（可选）：
learn.unfreeze()

关键技术点：

freeze / unfreeze（冻结与解冻）

学习率查找器（lr_find）

分层学习率（differential learning rates）





第八章：推荐系统（Collaborative Filtering）
核心概念：

协同过滤：通过用户与物品的交互（如评分）来预测兴趣。

使用 CollabDataLoaders 和 collab_learner 构建模型。

主要步骤：

加载数据：
dls = CollabDataLoaders.from_df(ratings, item_name='title')

创建模型：
learn = collab_learner(dls, y_range=(0, 5.5))

训练模型：
learn.fit_one_cycle(5, 5e-3)

关键概念：

嵌入（Embedding）：将用户和物品表示为向量。

潜在因子（Latent Factors）：模型自动学习的用户/物品特征。

MSE Loss：用于评分预测的回归损失函数。

PCA 可视化：用主成分分析降维并可视化物品之间的相似性。

进阶内容：

将嵌入向量拼接后输入神经网络替代点积。

使用正则化防止过拟合。


第9章
核心概念
Tabular 数据

指以行和列表示的数据，如 CSV 文件、数据库表格。

特征可以是 数值型（continuous） 或 类别型（categorical）。

嵌入（Embedding）用于类别变量

类似第8章的用户/电影嵌入。

每个类别（如“星期几”、“城市名”）被映射为一个向量。

模型会自动学习每个类别的语义特征。

处理数值变量

数值型特征通常会进行标准化（normalization），以帮助模型更快收敛。

模型结构：TabularModel

将所有嵌入向量与数值变量连接（concatenate）成一个大向量，输入到全连接神经网络中。

网络结构：输入层 → 若干隐藏层（可带 Dropout）→ 输出层（用于分类或回归）

损失函数选择

分类任务使用交叉熵损失（CrossEntropyLoss）

回归任务使用均方误差（MSELoss）

评估指标

分类：准确率（Accuracy）

回归：均方误差、平均绝对误差等

模型解释：特征重要性 & Embedding 可视化

可以通过可视化嵌入向量，观察模型是否学到了时间结构等有意义的信息。


中文术语  英文术语  说明
表格数据  Tabular data  包括数值型和类别型特征的数据表
类别变量  Categorical variable  有固定取值的变量，如性别、城市名
数值变量  Continuous variable 可以连续取值的变量，如年龄、收入
嵌入      Embedding 用向量表示类别变量
标准化 Normalization 将数值变量调整为均值为0，方差为1
拼接  Concatenation 将多个张量连接成一个向量
全连接层  Fully Connected Layer 神经网络中的基本层结构
激活函数  Activation Function 如 ReLU，用于非线性变换
丢弃法（防过拟合） Dropout 正则化方法，随机丢弃部分神经元
回归  Regression  预测连续值的任务
分类  Classification  预测类别的任务
损失函数  Loss Function 评估模型预测结果与真实值的差距
准确率 Accuracy  分类常用评估指标
可视化 Visualization 可视化嵌入向量或特征的重要性